{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "#nltk.download('punkt_tab') # RUN WITH THIS ONCE\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Dataframe: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports csv data to pandas dataframe\n",
    "data = pd.read_csv('Data.csv', index_col=0)\n",
    "\n",
    "# removes NAN values from dataframe\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           statement   status  num_of_chars  \\\n",
      "0                                         oh my gosh  Anxiety            10   \n",
      "1  trouble sleeping, confused mind, restless hear...  Anxiety            64   \n",
      "2  All wrong, back off dear, forward doubt. Stay ...  Anxiety            78   \n",
      "3  I've shifted my focus to something else but I'...  Anxiety            61   \n",
      "4  I'm restless and restless, it's been a month n...  Anxiety            72   \n",
      "\n",
      "   num_of_sents                                       lc_statement  \\\n",
      "0             1                                         oh my gosh   \n",
      "1             2  trouble sleeping confused mind restless heart ...   \n",
      "2             2  all wrong back off dear forward doubt stay in ...   \n",
      "3             1  ive shifted my focus to something else but im ...   \n",
      "4             2  im restless and restless its been a month now ...   \n",
      "\n",
      "                                              tokens  \\\n",
      "0                                     [oh, my, gosh]   \n",
      "1  [trouble, sleeping, confused, mind, restless, ...   \n",
      "2  [all, wrong, back, off, dear, forward, doubt, ...   \n",
      "3  [ive, shifted, my, focus, to, something, else,...   \n",
      "4  [im, restless, and, restless, its, been, a, mo...   \n",
      "\n",
      "                                      tokens_stemmed  \n",
      "0                                         oh my gosh  \n",
      "1  troubl sleep confus mind restless heart all ou...  \n",
      "2  all wrong back off dear forward doubt stay in ...  \n",
      "3  ive shift my focu to someth els but im still w...  \n",
      "4  im restless and restless it been a month now b...  \n"
     ]
    }
   ],
   "source": [
    "# function used to remove unwanted patterns (url, punctuation, etc)\n",
    "def rPatterns(text):\n",
    "    # removes URLs\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "\n",
    "    # removes mark-down links\n",
    "    text = re.sub(r'\\[.*?\\]\\(.*?\\)', '', text)\n",
    "\n",
    "    # removes handles\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "    # removes punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# used for stemming words\n",
    "def stem_words(words):\n",
    "    return ' '.join(stemmer.stem(str(word)) for word in words)\n",
    "\n",
    "# main code used for cleaning up csv:\n",
    "# adds columns for number of sentences and characters\n",
    "data['num_of_chars'] = data['statement'].str.len()\n",
    "data['num_of_sents'] = data['statement'].apply(lambda x: len(nltk.sent_tokenize(x)))\n",
    "\n",
    "# changes everything to lowercase\n",
    "data['lc_statement'] = data['statement'].str.lower()\n",
    "\n",
    "# removes unwanted text (punctuation, urls, etc.)\n",
    "data['lc_statement'] = data['lc_statement'].apply(rPatterns)\n",
    "\n",
    "# performs tokenization (split sentences into list of words)\n",
    "data['tokens'] = data['lc_statement'].apply(word_tokenize)\n",
    "\n",
    "# performs stemming (reduce words to their base/root form)\n",
    "stemmer = PorterStemmer()\n",
    "data['tokens_stemmed'] = data['tokens'].apply(stem_words)\n",
    "\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for machine learning models: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate features and labels\n",
    "X = data['tokens_stemmed']\n",
    "y = data['status']\n",
    "\n",
    "# label encoding target variable\n",
    "l_enc = LabelEncoder()\n",
    "y = l_enc.fit_transform(y.values)\n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "\n",
    "# convert text to features using TF-IDF\n",
    "vect = TfidfVectorizer(ngram_range=(1,2), max_features=50000)\n",
    "X_train_vect = vect.fit_transform(X_train)\n",
    "X_test_vect = vect.transform(X_test)\n",
    "\n",
    "# resample\n",
    "ros = RandomOverSampler(random_state=101)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train_vect, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Predict, and Score: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines classifiers and stores them in a dictionary\n",
    "classifiers = {\n",
    "    'Bernoulli' : BernoulliNB(alpha=0.1, binarize=0.0),\n",
    "    'DecTree' : DecisionTreeClassifier(max_depth=9, min_samples_split=5, random_state=101),\n",
    "    'LogReg' : LogisticRegression(solver='liblinear', penalty='l1', C=10, random_state=101)\n",
    "}\n",
    "\n",
    "# holds accuracy scores in case we want them for comparisons\n",
    "accuracy_scores = []\n",
    "\n",
    "# trains, tests, and evalutates for all classifiers\n",
    "for name, clf, in classifiers.items():\n",
    "    clf.fit(X_train_resampled, y_train_resampled)\n",
    "    y_pred = clf.predict(X_test_vect)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # prints accuracy score and classification report\n",
    "    print('')\n",
    "    print(f\"For {name}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    # creates confusion matrix and classification report\n",
    "    labels = l_enc.classes_\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(classification_report(y_test, y_pred, target_names=labels))\n",
    "\n",
    "    # prints confusion matrix\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='viridis', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix for {name}')\n",
    "    plt.show()\n",
    "\n",
    "    # adds accuracy score to list\n",
    "    accuracy_scores.append(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing just LogisticRegression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7595140931954066\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver='liblinear', penalty='l1', C=10, random_state=101)\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred = model.predict(X_test_vect)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving models as pickle files for usage in other files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model_pkl', 'wb') as files:\n",
    "    pickle.dump(model, files)\n",
    "\n",
    "with open('tfidf_pkl', 'wb') as file:\n",
    "    pickle.dump(vect, file)\n",
    "\n",
    "with open('labels_pkl', 'wb') as file:\n",
    "    pickle.dump(l_enc, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs325",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
